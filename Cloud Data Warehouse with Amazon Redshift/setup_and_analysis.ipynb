{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# AWS SETUP  \n",
    "\n",
    "In this notebook, we will use an sdk python library called \"boto3\" to create Redshift cluster and explore the the data in S3 bucket to further create ETL pipelines and DB objects. \n",
    "\n",
    "## (Prerequisite) Save the AWS Access key: \n",
    "\n",
    "### 1. Create a new IAM user\n",
    "IAM service is a global service, meaning newly created IAM users are not restricted to a specific region by default.\n",
    "- Go to [AWS IAM service](https://console.aws.amazon.com/iam/home#/users) and click on the \"**Add user**\" button to create a new IAM user in your AWS account. \n",
    "- Choose a name of your choice. \n",
    "- Select \"*Programmatic access*\" as the access type. Click Next. \n",
    "- Choose the *Attach existing policies directly* tab, and select the \"**AdministratorAccess**\". Click Next. \n",
    "- Skip adding any tags. Click Next. \n",
    "- Review and create the user. It will show you a pair of access key ID and secret.\n",
    "- Take note of the pair of access key ID and secret. This pair is collectively known as **Access key**. \n",
    "\n",
    "<center>\n",
    "<img style=\"float: center;height:300px;\" src=\"images/AWS_IAM_1.png\"><br><br>\n",
    "Snapshot of a pair of an Access key\n",
    "</center>\n",
    "\n",
    "### <font color='red'>2. Save the access key and secret</font>\n",
    "Edit the file `dwh.cfg` in the same folder as this notebook and save the access key and secret against the following variables:\n",
    "```bash\n",
    "KEY= <YOUR_AWS_KEY>\n",
    "SECRET= <YOUR_AWS_SECRET>\n",
    "```\n",
    "\n",
    "# Creating Redshift Cluster using the AWS python SDK \n",
    "We will perform the following steps.  \n",
    "- make a DWH.cfg file containg all configuration parameters. \n",
    "- create EC2 and S3 resource objects and also the client IAM and Redshift cluster objects to connect to resources \n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly) and attach policy and get IAM role ARN (Amazon Resource Names (ARNs) uniquely identify AWS resources)\n",
    "- Create a RedShift Cluster.\n",
    "- Open an incoming  TCP port to access the cluster ednpoint\n",
    "- connect to the cluster database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import required libraries \n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load DWH Params from dwh_init.cfg \n",
    "- The file is blank for security_purpose.\n",
    "- Fill yours AWS key account ( put these blank when in apublic repo) and userdefined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# make parsing object\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh_init.cfg'))\n",
    "\n",
    "# get keys with AWS block\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "# get keys with CLUSTER block\n",
    "DWH_CLUSTER_TYPE       = config.get(\"CLUSTER\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"CLUSTER\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"CLUSTER\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"CLUSTER\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"CLUSTER\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"CLUSTER\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"CLUSTER\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"CLUSTER\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"CLUSTER\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# get key from S3 block\n",
    "LOG_DATA              = config.get(\"S3\", \"LOG_DATA\")\n",
    "SONG_DATA             = config.get(\"S3\", \"SONG_DATA\")\n",
    "LOG_JSONPATH          = config.get(\"S3\", \"LOG_JSONPATH\")\n",
    "\n",
    "# Make a DataFrame\n",
    "pd.DataFrame({\"Parameters\":[\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"], \n",
    "              \"Values\":[DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create resource objects for EC2, S3, and client objects for IAM, and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "ec2 = boto3.resource('ec2',\n",
    "                      region_name=\"us-west-2\",\n",
    "                      aws_access_key_id=KEY,\n",
    "                      aws_secret_access_key=SECRET\n",
    "                    )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                     region_name=\"us-west-2\",\n",
    "                     aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                    region_name=\"us-west-2\",\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                         region_name=\"us-west-2\",\n",
    "                         aws_access_key_id=KEY,\n",
    "                         aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## connection to the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_bucket =  s3.Bucket(\"udacity-dend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket.objectsCollectionManager(s3.Bucket(name='udacity-dend'), s3.ObjectSummary)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_bucket.objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Above code shows we are able to access the s3 bucket 'udacity-dend'. can be shown with s3.Bucket.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Implement the logic in etl.py to load data from S3 to `staging tables` on Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###  Analysis of log_data and song_data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### staging tables names:\n",
    "- staging_events_table_create, \n",
    "- staging_songs_table_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Download a sample key object files from S3 to local for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!touch ./data/log_file\n",
    "!touch ./data/song_file\n",
    "!touch ./data/log_json_path.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./data/log_file', 'wb') as data:\n",
    "    dataset_bucket.download_fileobj('log-data/2018/11/2018-11-01-events.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./data/song_file', 'wb') as data:\n",
    "    dataset_bucket.download_fileobj('song-data/A/A/A/TRAAAAK128F9318786.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./data/log_json_path.json', 'wb') as data:\n",
    "    dataset_bucket.download_fileobj('log_json_path.json', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Use pandas for the analysis of file data types for use in staging Table creation \n",
    "- To check the datatypes in staging tables\n",
    "- for also be used in data modelling in Star schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_log = pd.read_json('./data/log_file', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist           False\n",
       "auth             True \n",
       "firstName        True \n",
       "gender           True \n",
       "itemInSession    True \n",
       "lastName         True \n",
       "length           False\n",
       "level            True \n",
       "location         True \n",
       "method           True \n",
       "page             True \n",
       "registration     True \n",
       "sessionId        True \n",
       "song             False\n",
       "status           True \n",
       "ts               True \n",
       "userAgent        True \n",
       "userId           True \n",
       "dtype: bool"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log.isnull().sum() == 0 # shows of only one Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist           object \n",
       "auth             object \n",
       "firstName        object \n",
       "gender           object \n",
       "itemInSession    int64  \n",
       "lastName         object \n",
       "length           float64\n",
       "level            object \n",
       "location         object \n",
       "method           object \n",
       "page             object \n",
       "registration     int64  \n",
       "sessionId        int64  \n",
       "song             object \n",
       "status           int64  \n",
       "ts               int64  \n",
       "userAgent        object \n",
       "userId           int64  \n",
       "dtype: object"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### for song_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_song = pd.read_json('./data/song_file', lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>num_songs</th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARJNIUY12298900C91</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adelitas Way</td>\n",
       "      <td>213.9424</td>\n",
       "      <td>1</td>\n",
       "      <td>SOBLFFE12AF72AA5BA</td>\n",
       "      <td>Scream</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id  artist_latitude artist_location  artist_longitude  \\\n",
       "0  ARJNIUY12298900C91 NaN                              NaN                 \n",
       "\n",
       "    artist_name  duration  num_songs             song_id   title  year  \n",
       "0  Adelitas Way  213.9424  1          SOBLFFE12AF72AA5BA  Scream  2009  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_song.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist_id           True \n",
       "artist_latitude     False\n",
       "artist_location     True \n",
       "artist_longitude    False\n",
       "artist_name         True \n",
       "duration            True \n",
       "num_songs           True \n",
       "song_id             True \n",
       "title               True \n",
       "year                True \n",
       "dtype: bool"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_song.isnull().sum() == 0 # shows of only one Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist_id           object \n",
       "artist_latitude     float64\n",
       "artist_location     object \n",
       "artist_longitude    float64\n",
       "artist_name         object \n",
       "duration            float64\n",
       "num_songs           int64  \n",
       "song_id             object \n",
       "title               object \n",
       "year                int64  \n",
       "dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_song.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Make connecection with Redshift Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### STEP 1: IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### STEP 2:  Redshift Cluster\n",
    "\n",
    "- Create a [RedShift Cluster](https://console.aws.amazon.com/redshiftv2/home)\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1 Describe the cluster to see its status\n",
    "- run this block several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'available'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<h2> 2.2 Take note of the cluster <font color='red'> endpoint and role ARN </font> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<font color='red'>DO NOT RUN THIS unless the cluster status becomes \"Available\". Make sure you are checking the Amazon Redshift cluster in the **us-west-2** region. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### STEP 3: Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# <font color='red'> Please Note</font>:  The below steps of this notebook are for analysis purpose for designing Table structures. The final analytics table and staging table structure is then included in sql_queries.py file of the project.  \n",
    "## <font color='red'>To delete cluster and release resoures, please go to the end of the file. </font>\n",
    "#  <font color='red'>  Please run create_tables.py and etl.py in console for the project.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Table design analysis \n",
    "## connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## ETL : Load data from S3 to staging tables on Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create staging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS staging_events_tbl;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS staging_events_tbl (\n",
    "artist           VARCHAR,\n",
    "auth             VARCHAR,\n",
    "firstName        VARCHAR,\n",
    "gender           CHAR(1),\n",
    "itemInSession    INTEGER,\n",
    "lastName         VARCHAR,\n",
    "length           DOUBLE PRECISION,\n",
    "level            VARCHAR(4),\n",
    "location         VARCHAR,\n",
    "method           CHAR(3),\n",
    "page             VARCHAR,\n",
    "registration     BIGINT,\n",
    "sessionId        INTEGER,\n",
    "song             VARCHAR,\n",
    "status           INTEGER,\n",
    "ts               BIGINT,\n",
    "userAgent        VARCHAR,\n",
    "userId           INTEGER\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS staging_songs_tbl;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS staging_songs_tbl (\n",
    "artist_id           VARCHAR,\n",
    "artist_latitude     DOUBLE PRECISION,\n",
    "artist_location     VARCHAR,\n",
    "artist_longitude    DOUBLE PRECISION,\n",
    "artist_name         VARCHAR,\n",
    "duration            DOUBLE PRECISION,\n",
    "num_songs           INTEGER,\n",
    "song_id             VARCHAR, \n",
    "title               VARCHAR,\n",
    "year                INTEGER\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load data into the staging tables in the cluster\n",
    "\n",
    "#### STEP 1: Get the params of the created redshift cluster\n",
    "- We need:\n",
    "    - The redshift cluster <font color='red'>endpoint</font>\n",
    "    - The <font color='red'>IAM role ARN</font> that give access to Redshift to read from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "https://docs.aws.amazon.com/redshift/latest/dg/r_COPY_command_examples.html#copy-from-json-examples-using-jsonpaths  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_events_copy = \"\"\"\n",
    "    copy staging_events_tbl\n",
    "    from {0}\n",
    "    credentials 'aws_iam_role={1}'\n",
    "    json {2} \n",
    "    region 'us-west-2'\n",
    "\"\"\".format(LOG_DATA, DWH_ROLE_ARN, LOG_JSONPATH )\n",
    "\n",
    "%sql $staging_events_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_songs_copy = \"\"\"\n",
    "    copy staging_songs_tbl\n",
    "    from {0}\n",
    "    credentials 'aws_iam_role={1}'\n",
    "    json 'auto' \n",
    "    region 'us-west-2'\n",
    "\"\"\".format(SONG_DATA, DWH_ROLE_ARN)\n",
    "\n",
    "%sql $staging_songs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from staging_events_tbl limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select * from staging_songs_tbl limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##  Analysis of staging tables  \n",
    "\n",
    "### Anlysis for staging songs Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select max(len(artist_id)) from staging_songs_tbl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select max(len(song_id)) from staging_songs_tbl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select max(artist_longitude), max(artist_latitude) from staging_songs_tbl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as artist_name_null_count from staging_songs_tbl where artist_name is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as title_null_count from  staging_songs_tbl where title is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as artist_id_null_count from staging_songs_tbl where artist_id is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as song_id_null_count from staging_songs_tbl where song_id is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as year_null_count from staging_songs_tbl where year is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as duration_null_count from staging_songs_tbl where duration is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as artist_latitude_null_count from staging_songs_tbl where artist_latitude is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as artist_longitude_null_count from staging_songs_tbl where artist_longitude is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select count(*) as artist_location_null_count from staging_songs_tbl where artist_location is NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select *  from staging_songs_tbl where artist_location is NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Analysis of staging_events_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### evaluation of not null constarints for firstname, lastname, level, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where firstname is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where lastname is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where gender is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where level is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where sessionid is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where location is null and userid is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql select  count(*) from staging_events_tbl where useragent in (null, 'None') and userid is not NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create analytics tables in Redshift (no distribution strategy) in the `nodist` schema and insertion into them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS nodist;\n",
    "DROP TABLE IF EXISTS nodist.time;\n",
    "DROP TABLE IF EXISTS nodist.artists;\n",
    "DROP TABLE IF EXISTS nodist.users;\n",
    "DROP TABLE IF EXISTS nodist.songs;\n",
    "DROP TABLE IF EXISTS nodist.songplays; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "CREATE TABLE IF NOT EXISTS nodist.time(\n",
    "start_time        TIMESTAMP     NOT NULL, \n",
    "hour              INTEGER       NOT NULL, \n",
    "day               INTEGER       NOT NULL, \n",
    "week              INTEGER       NOT NULL, \n",
    "month             INTEGER       NOT NULL, \n",
    "year              INTEGER       NOT NULL,\n",
    "weekday           INTEGER       NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS nodist.artists(\n",
    "artist_id         VARCHAR(18)   NOT NULL, \n",
    "name              VARCHAR       NOT NULL,\n",
    "location          TEXT,\n",
    "latitude          DOUBLE PRECISION,\n",
    "longitude         DOUBLE PRECISION\n",
    ");\n",
    "\n",
    "Create TABLE IF NOT EXISTS nodist.songs(\n",
    "song_id           VARCHAR(18)       NOT NULL,\n",
    "title             TEXT              NOT NULL, \n",
    "artist_id         VARCHAR(18)       NOT NULL,  \n",
    "year              INTEGER           NOT NULL, \n",
    "duration          DOUBLE PRECISION  NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS nodist.users(\n",
    "user_id           INT           NOT NULL,\n",
    "first_name        VARCHAR       NOT NULL,\n",
    "last_name         VARCHAR       NOT NULL,\n",
    "gender            CHAR(1)       NOT NULL,\n",
    "level             VARCHAR(4)    NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS nodist.songplays(\n",
    "songplay_id INT IDENTITY(0,1),\n",
    "start_time        TIMESTAMP     NOT NULL, \n",
    "user_id           INTEGER       NOT NULL,\n",
    "level             VARCHAR(4),\n",
    "song_id           VARCHAR(18),\n",
    "artist_id         VARCHAR(18),\n",
    "session_id        INTEGER       NOT NULL,\n",
    "location          TEXT          NOT NULL,\n",
    "user_agent        TEXT          NOT NULL\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select ts, \n",
    "timestamp 'epoch' + ts/1000 * interval '1 second' as start_time\n",
    "from staging_events_tbl limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO nodist.songplays(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "    SELECT  DISTINCT \n",
    "            timestamp 'epoch' + stg_event.ts/1000 * interval '1 second' as start_time,\n",
    "            stg_event.userid, \n",
    "            stg_event.level, \n",
    "            stg_song.song_id, \n",
    "            stg_song.artist_id, \n",
    "            stg_event.sessionid, \n",
    "            stg_event.location, \n",
    "            stg_event.useragent\n",
    "    FROM staging_events_tbl as stg_event JOIN staging_songs_tbl as stg_song ON  (\n",
    "            stg_event.song =  stg_song.title AND \n",
    "            stg_event.artist = stg_song.artist_name)\n",
    "    WHERE \n",
    "            stg_event.userid IS NOT NULL AND \n",
    "            stg_song.song_id IS NOT NULL AND \n",
    "            stg_song.artist_id IS NOT NULL AND\n",
    "            stg_event.page = 'NextSong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO nodist.time(start_time, hour, day, week, month, year, weekday)\n",
    "SELECT DISTINCT\n",
    "    start_time,\n",
    "    EXTRACT(hour    FROM start_time),\n",
    "    EXTRACT(day     FROM start_time),\n",
    "    EXTRACT(week    FROM start_time),\n",
    "    EXTRACT(month   FROM start_time),\n",
    "    EXTRACT(year    FROM start_time),\n",
    "    EXTRACT(weekday FROM start_time)\n",
    "FROM \n",
    "    nodist.songplays;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO nodist.artists(artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT\n",
    "    artist_id,\n",
    "    artist_name,\n",
    "    artist_location,\n",
    "    artist_latitude,\n",
    "    artist_longitude\n",
    "FROM\n",
    "    staging_songs_tbl\n",
    "WHERE \n",
    "    artist_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO nodist.users (user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT \n",
    "    userId, \n",
    "    firstName, \n",
    "    lastName, \n",
    "    gender, \n",
    "    level\n",
    "FROM \n",
    "    staging_events_tbl\n",
    "WHERE \n",
    "    page = 'NextSong' AND userId IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO nodist.songs (song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT \n",
    "    song_id, \n",
    "    title, \n",
    "    artist_id, \n",
    "    year, \n",
    "    duration\n",
    "FROM \n",
    "    staging_songs_tbl\n",
    "WHERE \n",
    "    song_id IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Analysis of size of dimension table for distributed and sort key determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM nodist.time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM nodist.artists;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM nodist.users;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM nodist.songs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM nodist.songplays;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Choose the best dist and sort key\n",
    "\n",
    "https://www.sisense.com/blog/double-your-redshift-performance-with-the-right-sortkeys-and-distkeys/  \n",
    "\n",
    "### Data redistribution\n",
    "\n",
    "When you load data into a table, Amazon Redshift distributes the rows of the table to each of the node slices according to the table's distribution style. As part of a query plan, the optimizer determines where blocks of data need to be located to best run the query. The data is then physically moved, or redistributed, while the query runs. Redistribution might involve either sending specific rows to nodes for joining or broadcasting an entire table to all of the nodes. \n",
    "\n",
    "\n",
    "\n",
    "### for sort key: \n",
    "- To have Amazon Redshift choose the appropriate sort order, specify AUTO for the sort key.\n",
    "\n",
    "- If recent data is queried most frequently, specify the timestamp column as the leading column for the sort key. Queries are more efficient because they can skip entire blocks that fall outside the time range.\n",
    "\n",
    "- If you do frequent range filtering or equality filtering on one column, specify that column as the sort key. Amazon Redshift can skip reading entire blocks of data for that column. It can do so because it tracks the minimum and maximum column values stored on each block and can skip blocks that don't apply to the predicate range.\n",
    "\n",
    "- If you frequently join a table, specify the join column as both the sort key and the distribution key. Doing this enables the query optimizer to choose a sort merge join instead of a slower hash join. Because the data is already sorted on the join key, the query optimizer can bypass the sort phase of the sort merge join.\n",
    "\n",
    "###  for dist key\n",
    "\n",
    "When you load data into a table, Amazon Redshift distributes the rows of the table to each of the node slices according to the table's distribution style. As part of a query plan, the optimizer determines where blocks of data need to be located to best run the query. The data is then physically moved, or redistributed, while the query runs. Redistribution might involve either sending specific rows to nodes for joining or broadcasting an entire table to all of the nodes. \n",
    "\n",
    "![Image](./images/distribution_rules.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### In our case :\n",
    "**Factors of deciding different distribution and sort key styles**:\n",
    "- songs table and artists table\n",
    " - We have decided our disribution style as KEY distribution for song table as the Table is big with respect to other however, we have not given the required analytical queries which must decide the distribution style. Also, we can see the artist table have similar but lower size but we take as AUTO since redshift will decide which should be the best distribution style for the artist table.\n",
    " - In addition, song table cotains inherently the artist_id column as well so it should be better taken as distribution style as Key distribution. \n",
    " - **sort key for songs table** is choosen as auto since their might be aggregations where compound keys which work better but in some case compound keys also fails (as per AWS docs). So we best choose AUTO to leave this decison on Redshift.\n",
    " - **sort key for artists table** is choosen as artist_id since it will work in agggregations and in cases without aggrgations. Year could be used but\n",
    " \n",
    "- Other tables:\n",
    " - Their size is rathe small so we implemented them with \"ALL distribution style\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We choose the star schema for our data modeling of Analytics tables in Redshift. Please find the complete analysis in `setup_and_analysis.ipynb` file. The below figure shows the our data model.\n",
    "![Image](./images/data_model.png)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create analytics tables in Redshift ( with distribution style) in the `nodist` schema and insertion into them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS dist;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "DROP TABLE IF EXISTS dist.time;\n",
    "DROP TABLE IF EXISTS dist.artists;\n",
    "DROP TABLE IF EXISTS dist.users;\n",
    "DROP TABLE IF EXISTS dist.songs;\n",
    "DROP TABLE IF EXISTS dist.songplays; \n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist.time(\n",
    "start_time        TIMESTAMP     NOT NULL   sortkey, \n",
    "hour              INTEGER       NOT NULL, \n",
    "day               INTEGER       NOT NULL, \n",
    "week              INTEGER       NOT NULL, \n",
    "month             INTEGER       NOT NULL, \n",
    "year              INTEGER       NOT NULL,\n",
    "weekday           INTEGER       NOT NULL)\n",
    "DISTSTYLE ALL;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist.artists(\n",
    "artist_id         VARCHAR(18)   NOT NULL   sortkey, \n",
    "name              VARCHAR       NOT NULL,\n",
    "location          TEXT,\n",
    "latitude          DOUBLE PRECISION,\n",
    "longitude         DOUBLE PRECISION)\n",
    "DISTSTYLE ALL;\n",
    "\n",
    "Create TABLE IF NOT EXISTS dist.songs(\n",
    "song_id           VARCHAR(18)       NOT NULL   distkey,\n",
    "title             TEXT              NOT NULL, \n",
    "artist_id         VARCHAR(18)       NOT NULL,  \n",
    "year              INTEGER           NOT NULL, \n",
    "duration          DOUBLE PRECISION  NOT NULL)\n",
    "SORTKEY AUTO;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist.users(\n",
    "user_id           INT           NOT NULL   sortkey,\n",
    "first_name        VARCHAR       NOT NULL,\n",
    "last_name         VARCHAR       NOT NULL,\n",
    "gender            CHAR(1)       NOT NULL,\n",
    "level             VARCHAR(4)    NOT NULL)\n",
    "DISTSTYLE ALL;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS dist.songplays(\n",
    "songplay_id INT IDENTITY(0,1)   NOT NULL,\n",
    "start_time        TIMESTAMP     NOT NULL, \n",
    "user_id           INTEGER       NOT NULL,\n",
    "level             VARCHAR(4),\n",
    "song_id           VARCHAR(18)   distkey,\n",
    "artist_id         VARCHAR(18),\n",
    "session_id        INTEGER       NOT NULL,\n",
    "location          TEXT          NOT NULL,\n",
    "user_agent        TEXT          NOT NULL)\n",
    "SORTKEY AUTO;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dist.songplays(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "    SELECT  DISTINCT \n",
    "            timestamp 'epoch' + stg_event.ts/1000 * interval '1 second' as start_time,\n",
    "            stg_event.userid, \n",
    "            stg_event.level, \n",
    "            stg_song.song_id, \n",
    "            stg_song.artist_id, \n",
    "            stg_event.sessionid, \n",
    "            stg_event.location, \n",
    "            stg_event.useragent\n",
    "    FROM staging_events_tbl as stg_event JOIN staging_songs_tbl as stg_song ON  (\n",
    "            stg_event.song =  stg_song.title AND \n",
    "            stg_event.artist = stg_song.artist_name)\n",
    "    WHERE \n",
    "            stg_event.userid IS NOT NULL AND \n",
    "            stg_song.song_id IS NOT NULL AND \n",
    "            stg_song.artist_id IS NOT NULL AND\n",
    "            stg_event.page = 'NextSong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dist.time(start_time, hour, day, week, month, year, weekday)\n",
    "SELECT DISTINCT\n",
    "    start_time,\n",
    "    EXTRACT(hour    FROM start_time),\n",
    "    EXTRACT(day     FROM start_time),\n",
    "    EXTRACT(week    FROM start_time),\n",
    "    EXTRACT(month   FROM start_time),\n",
    "    EXTRACT(year    FROM start_time),\n",
    "    EXTRACT(weekday FROM start_time)\n",
    "FROM \n",
    "    dist.songplays; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dist.artists(artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT\n",
    "    artist_id,\n",
    "    artist_name,\n",
    "    artist_location,\n",
    "    artist_latitude,\n",
    "    artist_longitude\n",
    "FROM\n",
    "    staging_songs_tbl\n",
    "WHERE \n",
    "    artist_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dist.users (user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT \n",
    "    userId, \n",
    "    firstName, \n",
    "    lastName, \n",
    "    gender, \n",
    "    level\n",
    "FROM \n",
    "    staging_events_tbl\n",
    "WHERE \n",
    "    page = 'NextSong' AND userId IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO dist.songs (song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT \n",
    "    song_id, \n",
    "    title, \n",
    "    artist_id, \n",
    "    year, \n",
    "    duration\n",
    "FROM \n",
    "    staging_songs_tbl\n",
    "WHERE \n",
    "    song_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Comparison between  nodist and dist schemas with sample queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Query 1: Top 10 states with most number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "SELECT \n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \n",
    "FROM \n",
    "nodist.songplays \n",
    "group by state order by count desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "SELECT \n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \n",
    "FROM \n",
    "dist.songplays \n",
    "group by state order by count desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query 2: 10 states with least number of user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \\\n",
    "FROM \\\n",
    "nodist.songplays \\\n",
    "group by state order by count LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \\\n",
    "FROM \\\n",
    "dist.songplays \\\n",
    "group by state order by count LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query 3: Statewise Free vs paid user count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \\\n",
    "FROM \\\n",
    "nodist.songplays \\\n",
    "group by state order by count LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state , count(SPLIT_PART(Location, ',' , 2)) \\\n",
    "FROM \\\n",
    "dist.songplays \\\n",
    "group by state order by count LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Query 4:  Statewise Free vs paid user count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state, level, count(SPLIT_PART(Location, ',' , 2))  \\\n",
    "FROM nodist.songplays \\\n",
    "group by state, level order by SPLIT_PART(Location, ',' , 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state, level, count(SPLIT_PART(Location, ',' , 2))  \\\n",
    "FROM dist.songplays \\\n",
    "group by state, level order by SPLIT_PART(Location, ',' , 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Query 5:  Genderwise distribution of users in different states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state, users.gender, count(users.gender)  \\\n",
    "FROM nodist.songplays JOIN nodist.users ON songplays.user_id =  users.user_id \\\n",
    "GROUP BY state, gender \\\n",
    "order by SPLIT_PART(Location, ',' , 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%sql SELECT \\\n",
    "SPLIT_PART(Location, ',' , 2) as state, users.gender, count(users.gender)  \\\n",
    "FROM dist.songplays JOIN nodist.users ON songplays.user_id =  users.user_id \\\n",
    "GROUP BY state, gender \\\n",
    "order by SPLIT_PART(Location, ',' , 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### query 6: top songs users listening to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "select \n",
    "    so.title, count(sp.user_id) as user_count\n",
    "FROM \n",
    "    nodist.songplays sp join nodist.songs so \n",
    "    ON sp.song_id = so.song_id \n",
    "group by \n",
    "    so.title\n",
    "order by \n",
    "    user_count desc\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "select \n",
    "    so.title, count(sp.user_id) as user_count\n",
    "FROM \n",
    "    dist.songplays sp join nodist.songs so \n",
    "    ON sp.song_id = so.song_id \n",
    "group by \n",
    "    so.title\n",
    "order by \n",
    "    user_count desc\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "nodist_time = np.array([5.91, 6.28, 4.96, 5.74, 6.68, 5.68])\n",
    "dist_time = np.array([ 4.96,4.36, 4.51, 6.30, 6.16, 4.63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "percentage_change_wrt_nodist = (nodist_time - dist_time) / nodist_time * 100 \n",
    "x = np.arange(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f135c362208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x, percentage_change_wrt_nodist, width= 0.1)\n",
    "plt.title(\"Percentage change in distributed database wrt undistributed\")\n",
    "plt.xticks([0, 1,2,3,4,5])\n",
    "plt.xlabel(\"Query number\")\n",
    "plt.ylabel( \"Percentage change\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### By above queries we see, we observe that we have slight improvement in most of the queries inspite of the fact that dimension and song_plyas table contain decent number of rows. Therefore, we can go forward with our dist schema style to implement in our project `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE <br/> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Clean up your resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "#redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### run the below block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "redshift.describe_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "#iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "#iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
